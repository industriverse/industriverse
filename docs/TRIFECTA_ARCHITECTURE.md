# TRIFECTA ARCHITECTURE: UserLM + RND1 + ACE Integration

**Date**: November 21, 2025
**Purpose**: Map existing Thermodynasty ACE to full Trifecta multi-agent intelligence system
**Status**: Design Document - Ready for Implementation

---

## ğŸ¯ EXECUTIVE SUMMARY

The **Trifecta** is a three-agent cognitive architecture that extends the existing ACE (Aspiration-Calibration-Execution) framework with two additional specialized agents:

1. **ACE** (Aspiration-Calibration-Execution) - âœ… **ALREADY IMPLEMENTED** in Phase 4
   - **File**: `src/core_ai_layer/nvp/ace/ace_agent.py` (530 lines)
   - **Purpose**: Goal-directed prediction with metacognitive capabilities
   - **Status**: Production-ready with Shadow Ensemble BFT consensus

2. **UserLM** (User Language Model) - âŒ **TO BE IMPLEMENTED**
   - **Purpose**: Human behavior simulation, persona generation, red-team modeling
   - **Location**: `src/trifecta/userlm/`
   - **Status**: New component

3. **RND1** (Resource + Defense Optimizer) - âŒ **TO BE IMPLEMENTED**
   - **Purpose**: Evolutionary resource optimization, self-evolving defense strategies
   - **Location**: `src/trifecta/rnd1/`
   - **Status**: New component

---

## ğŸ“ ARCHITECTURE OVERVIEW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRIFECTA LAYER                              â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚    UserLM    â”‚      â”‚     RND1     â”‚      â”‚     ACE      â”‚    â”‚
â”‚  â”‚  (NEW)       â”‚â—„â”€â”€â”€â”€â–ºâ”‚   (NEW)      â”‚â—„â”€â”€â”€â”€â–ºâ”‚  (EXISTS)    â”‚    â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚              â”‚    â”‚
â”‚  â”‚ â€¢ Persona    â”‚      â”‚ â€¢ Resource   â”‚      â”‚ â€¢ Aspiration â”‚    â”‚
â”‚  â”‚   Generator  â”‚      â”‚   Optimizer  â”‚      â”‚ â€¢ Calibrationâ”‚    â”‚
â”‚  â”‚ â€¢ Behavior   â”‚      â”‚ â€¢ Defense    â”‚      â”‚ â€¢ Execution  â”‚    â”‚
â”‚  â”‚   Simulator  â”‚      â”‚   Evolver    â”‚      â”‚ â€¢ NVP Model  â”‚    â”‚
â”‚  â”‚ â€¢ Red Team   â”‚      â”‚ â€¢ Attack     â”‚      â”‚ â€¢ Shadow     â”‚    â”‚
â”‚  â”‚   Agent      â”‚      â”‚   Predictor  â”‚      â”‚   Ensemble   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                     â”‚                      â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                               â”‚                                   â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                    â”‚  TRIFECTA CORTEX    â”‚                       â”‚
â”‚                    â”‚  (Coordination Hub)  â”‚                       â”‚
â”‚                    â”‚  â€¢ Memory Manager    â”‚                       â”‚
â”‚                    â”‚  â€¢ Playbook Engine   â”‚                       â”‚
â”‚                    â”‚  â€¢ Inter-Agent Comm  â”‚                       â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   BRIDGE API (MCP)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  EIL (Phase 5)       â”‚
                    â”‚  â€¢ MicroAdapt v2     â”‚
                    â”‚  â€¢ Shadow Ensemble   â”‚
                    â”‚  â€¢ Market Engine     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” EXISTING ACE IMPLEMENTATION ANALYSIS

### Current ACE Agent Architecture

**File**: `src/core_ai_layer/nvp/ace/ace_agent.py:407-530`

The existing ACE agent implements a three-layer cognitive architecture:

#### 1. **Aspiration Layer** (Lines 118-185)
```python
class AspirationLayer:
    """Goal Setting and Target State Selection"""

    def __init__(self, config: AspirationConfig):
        self.goals = {
            'energy_fidelity': config.target_energy_fidelity,    # 0.95
            'entropy_coherence': config.target_entropy_coherence, # 0.90
            'rmse': config.target_rmse,                          # 0.1
            'confidence': config.min_confidence                  # 0.7
        }

    def assess_achievement(self, result: PredictionResult) -> Dict[str, bool]:
        """Assess whether goals were achieved"""
        # Returns achievement status for each goal

    def adjust_goals(self, result: PredictionResult, factor: float = 0.9):
        """Socratic refinement: Adjust goals based on performance"""
```

**Key Capabilities**:
- âœ… Goal setting with thermodynamic constraints
- âœ… Achievement assessment
- âœ… Dynamic goal adjustment (Socratic loop)

#### 2. **Calibration Layer** (Lines 187-283)
```python
class CalibrationLayer:
    """Confidence Estimation and Uncertainty Quantification"""

    def estimate_uncertainty(self, mean_pred, log_var_pred) -> Tuple[np.ndarray, float]:
        """Estimate prediction uncertainty from model outputs"""
        # Returns uncertainty map + confidence score

    def ensemble_consensus(self, predictions: List[np.ndarray]) -> Tuple:
        """BFT (Byzantine Fault Tolerance) consensus from ensemble"""
        # Returns consensus_pred, agreement score, consensus_valid
```

**Key Capabilities**:
- âœ… Uncertainty estimation (entropy or variance method)
- âœ… BFT consensus (2/3 majority, median-based)
- âœ… Agreement scoring

#### 3. **Execution Layer** (Lines 286-405)
```python
class ExecutionLayer:
    """NVP Model Inference"""

    def __init__(self, config: ExecutionConfig, model_path: Optional[Path] = None, rng=None):
        self.model = NVPModel(config.nvp_config)
        self.state = create_train_state(...)

    def predict(self, energy_t, grad_x, grad_y) -> Tuple[np.ndarray, np.ndarray]:
        """Execute NVP prediction"""
        # Returns mean_pred, log_var_pred

    def _enforce_energy_conservation(self, prediction, reference) -> np.ndarray:
        """Enforce energy conservation by rescaling prediction"""
```

**Key Capabilities**:
- âœ… NVP model inference (Next Vector Prediction)
- âœ… Thermodynamic constraint enforcement
- âœ… Checkpoint loading

#### 4. **ACE Agent** (Lines 407-530)
```python
class ACEAgent:
    """Complete ACE (Aspiration-Calibration-Execution) Agent"""

    def __init__(self, config: ACEConfig, model_path: Optional[Path] = None):
        self.aspiration = AspirationLayer(config.aspiration)
        self.calibration = CalibrationLayer(config.calibration, cal_rng)
        self.execution = ExecutionLayer(config.execution, model_path, exec_rng)
        self.history: List[PredictionResult] = []

    def predict(self, energy_t, grad_x, grad_y, energy_target=None) -> PredictionResult:
        """Make a prediction using the full ACE architecture"""
        # 1. EXECUTION: Run NVP model
        # 2. CALIBRATION: Estimate uncertainty and confidence
        # 3. ASPIRATION: Check if goals met
        return PredictionResult(...)
```

**Key Capabilities**:
- âœ… Unified three-layer prediction pipeline
- âœ… Prediction history tracking
- âœ… Energy fidelity computation
- âœ… Entropy coherence computation

### Shadow Ensemble Integration

**File**: `src/core_ai_layer/nvp/ace/shadow_ensemble.py:74-100`

```python
class ShadowEnsemble:
    """Shadow Ensemble of NVP Models with BFT Consensus"""

    def __init__(self, config: EnsembleConfig, ace_config: ACEConfig, model_paths=None):
        self.models: List[ExecutionLayer] = []  # 3 independent models
        # Creates 3 diversified models for BFT
```

**Key Capabilities**:
- âœ… Byzantine Fault Tolerance (2/3 majority)
- âœ… Median-based consensus (robust to outliers)
- âœ… Ensemble disagreement quantification
- âœ… Faulty model detection

---

## ğŸ†• NEW COMPONENTS TO IMPLEMENT

### 1. UserLM (User Language Model)

**Purpose**: Simulate human behavior, generate personas, red-team adversarial scenarios

**File Structure**:
```
src/trifecta/userlm/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ persona_generator.py       # Generate synthetic user personas
â”œâ”€â”€ behavior_simulator.py      # Simulate user actions/decisions
â”œâ”€â”€ red_team_agent.py          # Adversarial scenario generation
â”œâ”€â”€ dialogue_engine.py         # Multi-turn conversation simulation
â”œâ”€â”€ intent_classifier.py       # Classify user intent from behavior
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ persona_model.py       # LLM-based persona model
â”‚   â”œâ”€â”€ behavior_patterns.py   # Behavior pattern library
â”‚   â””â”€â”€ attack_vectors.py      # Red-team attack vector database
â””â”€â”€ tests/
    â”œâ”€â”€ test_persona_generator.py
    â”œâ”€â”€ test_behavior_simulator.py
    â””â”€â”€ test_red_team_agent.py
```

#### Core Components

##### A. Persona Generator (`persona_generator.py`)
```python
@dataclass
class UserPersona:
    """Synthetic user persona"""
    persona_id: str
    demographics: Dict[str, Any]  # age, location, occupation, etc.
    behavioral_traits: Dict[str, float]  # risk_aversion, tech_savvy, etc.
    goals: List[str]  # User objectives
    constraints: List[str]  # User limitations
    attack_profile: Optional[Dict[str, Any]] = None  # If adversarial

class PersonaGenerator:
    """Generate diverse synthetic user personas"""

    def __init__(self, llm_backend: str = "gpt-4"):
        self.llm = self._init_llm(llm_backend)
        self.persona_library: Dict[str, UserPersona] = {}

    def generate_persona(
        self,
        persona_type: str = "benign",  # "benign", "adversarial", "mixed"
        seed: Optional[int] = None
    ) -> UserPersona:
        """Generate a synthetic user persona"""
        # Use LLM to generate realistic persona
        # Store behavioral traits, goals, constraints

    def generate_adversarial_persona(
        self,
        attack_vector: str = "fraud"  # "fraud", "data_theft", "dos", etc.
    ) -> UserPersona:
        """Generate adversarial persona for red-teaming"""
        # Generate attacker profile with tactics, techniques, procedures (TTPs)
```

**Integration with ACE**:
- ACE provides confidence scores â†’ UserLM uses them to model user trust
- UserLM generates personas â†’ ACE tests predictions against persona behaviors

##### B. Behavior Simulator (`behavior_simulator.py`)
```python
@dataclass
class BehaviorTrace:
    """Trace of simulated user behavior"""
    persona_id: str
    timestamp: float
    action_sequence: List[Dict[str, Any]]
    energy_signature: np.ndarray  # Thermodynamic signature of behavior
    anomaly_score: float

class BehaviorSimulator:
    """Simulate user behavior based on persona"""

    def __init__(self, ace_agent: ACEAgent):
        self.ace_agent = ace_agent  # Use ACE for behavior prediction
        self.behavior_history: List[BehaviorTrace] = []

    def simulate_behavior(
        self,
        persona: UserPersona,
        scenario: Dict[str, Any],
        num_steps: int = 10
    ) -> BehaviorTrace:
        """Simulate multi-step user behavior"""
        # Use ACE to predict next behavior state
        # Apply persona constraints and goals
        # Generate thermodynamic signature of behavior

    def detect_anomaly(self, trace: BehaviorTrace) -> float:
        """Detect anomalous behavior using ACE confidence"""
        # Use ACE calibration layer to compute anomaly score
```

**Integration with ACE**:
- Reuses ACE's Calibration Layer for anomaly detection
- Uses ACE's prediction uncertainty to model behavioral uncertainty

##### C. Red Team Agent (`red_team_agent.py`)
```python
@dataclass
class AttackScenario:
    """Adversarial attack scenario"""
    scenario_id: str
    attack_type: str  # "account_takeover", "fraud", "data_theft", etc.
    persona: UserPersona
    tactics: List[str]  # MITRE ATT&CK tactics
    expected_detection_rate: float

class RedTeamAgent:
    """Generate and execute adversarial scenarios"""

    def __init__(self, userlm: PersonaGenerator, ace_agent: ACEAgent):
        self.userlm = userlm
        self.ace_agent = ace_agent
        self.attack_library: Dict[str, AttackScenario] = {}

    def generate_attack_scenario(
        self,
        attack_type: str,
        sophistication: float = 0.5  # 0.0 = basic, 1.0 = APT-level
    ) -> AttackScenario:
        """Generate adversarial attack scenario"""
        # Generate adversarial persona
        # Define attack tactics and techniques
        # Predict expected detection rate using ACE

    def execute_attack_simulation(
        self,
        scenario: AttackScenario,
        target_system: Any
    ) -> Dict[str, Any]:
        """Execute simulated attack and measure defenses"""
        # Simulate attack behavior
        # Monitor ACE confidence and anomaly scores
        # Return attack success metrics
```

**Integration with ACE**:
- Uses ACE's Aspiration Layer to set attack goals
- Uses ACE's Calibration Layer to measure defense effectiveness

---

### 2. RND1 (Resource + Defense Optimizer)

**Purpose**: Evolutionary resource optimization, self-evolving defense strategies, attack prediction

**File Structure**:
```
src/trifecta/rnd1/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ resource_optimizer.py      # Optimize resource allocation
â”œâ”€â”€ defense_evolver.py         # Evolve defense strategies
â”œâ”€â”€ attack_predictor.py        # Predict future attack vectors
â”œâ”€â”€ evolutionary_engine.py     # Evolutionary algorithm core
â”œâ”€â”€ fitness_functions.py       # Multi-objective fitness functions
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ resource_model.py      # Resource allocation model
â”‚   â”œâ”€â”€ defense_library.py     # Library of defense strategies
â”‚   â””â”€â”€ attack_patterns.py     # Known attack pattern database
â””â”€â”€ tests/
    â”œâ”€â”€ test_resource_optimizer.py
    â”œâ”€â”€ test_defense_evolver.py
    â””â”€â”€ test_attack_predictor.py
```

#### Core Components

##### A. Resource Optimizer (`resource_optimizer.py`)
```python
@dataclass
class ResourceAllocation:
    """Resource allocation configuration"""
    allocation_id: str
    resources: Dict[str, float]  # CPU, memory, GPU, network, etc.
    objectives: Dict[str, float]  # throughput, latency, cost, etc.
    constraints: Dict[str, float]  # budget, capacity, etc.
    energy_efficiency: float  # Thermodynamic efficiency

class ResourceOptimizer:
    """Optimize resource allocation using evolutionary algorithms"""

    def __init__(self, ace_agent: ACEAgent):
        self.ace_agent = ace_agent  # Use ACE for optimization guidance
        self.allocation_history: List[ResourceAllocation] = []

    def optimize_allocation(
        self,
        current_allocation: ResourceAllocation,
        objectives: Dict[str, float],
        constraints: Dict[str, float]
    ) -> ResourceAllocation:
        """Optimize resource allocation"""
        # Use evolutionary algorithm to search allocation space
        # Use ACE to predict energy cost of each allocation
        # Multi-objective optimization (Pareto frontier)

    def predict_resource_demand(
        self,
        workload_history: List[Dict[str, Any]],
        horizon: int = 10
    ) -> List[Dict[str, float]]:
        """Predict future resource demand using ACE"""
        # Convert workload to energy representation
        # Use ACE's NVP model to predict future demand
        # Return predicted resource requirements
```

**Integration with ACE**:
- Uses ACE's NVP model to predict future resource demand
- Uses ACE's Aspiration Layer to set optimization objectives
- Uses ACE's energy conservation to ensure thermodynamic efficiency

##### B. Defense Evolver (`defense_evolver.py`)
```python
@dataclass
class DefenseStrategy:
    """Defense strategy configuration"""
    strategy_id: str
    layers: List[Dict[str, Any]]  # Defense layers (AI Shield layers)
    parameters: Dict[str, float]  # Strategy parameters
    effectiveness: Dict[str, float]  # Effectiveness against attack types
    fitness_score: float  # Evolutionary fitness

class DefenseEvolver:
    """Evolve defense strategies using evolutionary algorithms"""

    def __init__(self, ace_agent: ACEAgent, red_team: RedTeamAgent):
        self.ace_agent = ace_agent
        self.red_team = red_team
        self.strategy_population: List[DefenseStrategy] = []

    def initialize_population(self, pop_size: int = 50):
        """Initialize population of defense strategies"""
        # Generate diverse initial population

    def evaluate_fitness(
        self,
        strategy: DefenseStrategy,
        attack_scenarios: List[AttackScenario]
    ) -> float:
        """Evaluate defense strategy fitness"""
        # Test strategy against red-team attack scenarios
        # Measure detection rate, false positive rate, latency
        # Use ACE confidence to assess defense certainty
        # Multi-objective fitness: detection + efficiency + cost

    def evolve_generation(self) -> List[DefenseStrategy]:
        """Evolve one generation of defense strategies"""
        # Selection: Tournament selection based on fitness
        # Crossover: Combine successful strategies
        # Mutation: Introduce random variations
        # Elitism: Keep top strategies

    def get_best_strategy(self) -> DefenseStrategy:
        """Get best evolved defense strategy"""
        return max(self.strategy_population, key=lambda s: s.fitness_score)
```

**Integration with ACE**:
- Uses ACE's Calibration Layer to measure defense confidence
- Uses ACE's Shadow Ensemble consensus to validate defense decisions
- Uses Red Team Agent (which uses ACE) to test evolved defenses

##### C. Attack Predictor (`attack_predictor.py`)
```python
@dataclass
class AttackPrediction:
    """Predicted future attack"""
    prediction_id: str
    attack_type: str
    predicted_time: float  # Estimated time of attack
    confidence: float  # ACE confidence score
    attack_vectors: List[str]  # Predicted attack vectors
    recommended_defenses: List[DefenseStrategy]

class AttackPredictor:
    """Predict future attack vectors using ACE + historical patterns"""

    def __init__(self, ace_agent: ACEAgent, defense_evolver: DefenseEvolver):
        self.ace_agent = ace_agent
        self.defense_evolver = defense_evolver
        self.attack_history: List[Dict[str, Any]] = []

    def predict_attack(
        self,
        current_state: Dict[str, Any],
        horizon: int = 10  # Prediction horizon (steps)
    ) -> AttackPrediction:
        """Predict future attack using ACE NVP"""
        # Convert attack history to energy representation
        # Use ACE's NVP model to predict next attack state
        # Use ACE's Calibration Layer to estimate confidence
        # Retrieve recommended defenses from defense evolver

    def analyze_threat_landscape(
        self,
        time_window: int = 30  # Days
    ) -> Dict[str, Any]:
        """Analyze evolving threat landscape"""
        # Analyze attack history patterns
        # Identify emerging attack vectors
        # Predict future threat trends using ACE
```

**Integration with ACE**:
- Uses ACE's NVP model to predict attack trajectories
- Uses ACE's Calibration Layer to estimate prediction confidence
- Uses ACE's energy representation for attack pattern analysis

---

## ğŸ”— TRIFECTA CORTEX: Inter-Agent Coordination

**Purpose**: Coordinate communication and decision-making across UserLM, RND1, and ACE

**File**: `src/trifecta/cortex/trifecta_cortex.py`

```python
@dataclass
class TrifectaMemory:
    """Shared memory across Trifecta agents"""
    personas: Dict[str, UserPersona]
    behaviors: Dict[str, BehaviorTrace]
    defenses: Dict[str, DefenseStrategy]
    predictions: Dict[str, PredictionResult]
    attacks: Dict[str, AttackScenario]

@dataclass
class PlaybookAction:
    """Action from a playbook"""
    action_id: str
    agent: str  # "userlm", "rnd1", "ace"
    action_type: str
    parameters: Dict[str, Any]
    expected_outcome: Dict[str, Any]

class TrifectaCortex:
    """
    Trifecta Cortex: Coordination Hub for UserLM + RND1 + ACE

    Manages:
    - Inter-agent communication
    - Shared memory
    - Playbook execution
    - Decision arbitration
    """

    def __init__(
        self,
        ace_agent: ACEAgent,
        userlm: PersonaGenerator,
        rnd1: ResourceOptimizer
    ):
        self.ace = ace_agent
        self.userlm = userlm
        self.rnd1 = rnd1

        # Shared memory
        self.memory = TrifectaMemory(
            personas={},
            behaviors={},
            defenses={},
            predictions={},
            attacks={}
        )

        # Playbook engine
        self.playbooks: Dict[str, List[PlaybookAction]] = {}

    def execute_playbook(
        self,
        playbook_name: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a multi-agent playbook

        Examples:
        - "red_team_defense_test": UserLM generates attack â†’ RND1 evolves defense â†’ ACE validates
        - "resource_optimization": ACE predicts demand â†’ RND1 optimizes allocation â†’ UserLM simulates load
        - "threat_prediction": RND1 predicts attack â†’ UserLM simulates adversary â†’ ACE estimates impact
        """
        playbook = self.playbooks[playbook_name]
        results = {}

        for action in playbook:
            if action.agent == "userlm":
                result = self._execute_userlm_action(action, context)
            elif action.agent == "rnd1":
                result = self._execute_rnd1_action(action, context)
            elif action.agent == "ace":
                result = self._execute_ace_action(action, context)
            else:
                raise ValueError(f"Unknown agent: {action.agent}")

            results[action.action_id] = result
            context.update(result)  # Pass result to next action

        return results

    def coordinate_decision(
        self,
        decision_type: str,
        candidates: List[Any]
    ) -> Any:
        """
        Coordinate multi-agent decision

        Examples:
        - "defense_selection": RND1 proposes defenses â†’ ACE evaluates confidence â†’ UserLM tests effectiveness
        - "resource_allocation": RND1 optimizes allocation â†’ ACE predicts energy cost â†’ UserLM simulates load
        """
        # Step 1: ACE evaluates each candidate (confidence, energy cost)
        ace_scores = [self._ace_evaluate(c) for c in candidates]

        # Step 2: RND1 evaluates each candidate (fitness, efficiency)
        rnd1_scores = [self._rnd1_evaluate(c) for c in candidates]

        # Step 3: UserLM evaluates each candidate (user impact, usability)
        userlm_scores = [self._userlm_evaluate(c) for c in candidates]

        # Step 4: Weighted combination (configurable weights)
        combined_scores = [
            0.4 * ace + 0.4 * rnd1 + 0.2 * userlm
            for ace, rnd1, userlm in zip(ace_scores, rnd1_scores, userlm_scores)
        ]

        # Step 5: Select best candidate
        best_idx = np.argmax(combined_scores)
        return candidates[best_idx]

    def update_memory(self, key: str, value: Any):
        """Update shared memory"""
        if hasattr(self.memory, key):
            getattr(self.memory, key).update(value)

    def sync_agents(self):
        """Synchronize state across all agents"""
        # Share ACE predictions with RND1 and UserLM
        # Share RND1 defenses with ACE and UserLM
        # Share UserLM personas with ACE and RND1
```

---

## ğŸ”„ INTEGRATION PATTERNS

### Pattern 1: Red-Team Defense Evolution Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UserLM    â”‚ Generate adversarial persona
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ persona
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UserLM    â”‚ Simulate attack behavior
â”‚ (Red Team)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ attack_trace
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RND1    â”‚ Evaluate current defense effectiveness
â”‚  (Defense)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ defense_gaps
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RND1    â”‚ Evolve new defense strategies
â”‚  (Evolver)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ defense_candidates
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ACE     â”‚ Validate defense confidence
â”‚(Calibration)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ validated_defense
       â–¼
     [Deploy]
```

**Code Example**:
```python
# Red-Team Defense Evolution Loop
def red_team_defense_loop(cortex: TrifectaCortex, iterations: int = 10):
    for i in range(iterations):
        # Step 1: UserLM generates adversarial persona
        attacker = cortex.userlm.generate_adversarial_persona(attack_vector="fraud")

        # Step 2: UserLM simulates attack
        attack_trace = cortex.userlm.simulate_attack(attacker)

        # Step 3: RND1 evaluates current defenses
        defense_gaps = cortex.rnd1.evaluate_defense_gaps(attack_trace)

        # Step 4: RND1 evolves new defenses
        new_defenses = cortex.rnd1.evolve_defenses(defense_gaps)

        # Step 5: ACE validates defense confidence
        validated = cortex.ace.validate_defense(new_defenses)

        # Step 6: Deploy best defense
        best_defense = max(validated, key=lambda d: d.confidence)
        cortex.memory.defenses[f"defense_gen_{i}"] = best_defense
```

### Pattern 2: Resource Optimization with User Simulation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ACE     â”‚ Predict future workload (NVP)
â”‚    (NVP)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ workload_prediction
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RND1    â”‚ Optimize resource allocation
â”‚ (Optimizer) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ allocation_plan
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UserLM    â”‚ Simulate user load with allocation
â”‚ (Simulator) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ simulated_performance
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ACE     â”‚ Validate energy efficiency
â”‚ (Aspiration)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ validated_allocation
       â–¼
     [Deploy]
```

**Code Example**:
```python
# Resource Optimization with User Simulation
def optimize_resources_with_simulation(cortex: TrifectaCortex, workload_history: List):
    # Step 1: ACE predicts future workload
    workload_pred = cortex.ace.predict_workload(workload_history, horizon=24)

    # Step 2: RND1 optimizes resource allocation
    allocation_plan = cortex.rnd1.optimize_allocation(
        current_resources=get_current_resources(),
        predicted_demand=workload_pred,
        objectives={"latency": 0.9, "cost": 0.8}
    )

    # Step 3: UserLM simulates user load
    personas = [cortex.userlm.generate_persona() for _ in range(100)]
    simulated_perf = cortex.userlm.simulate_load(personas, allocation_plan)

    # Step 4: ACE validates energy efficiency
    energy_efficiency = cortex.ace.compute_energy_fidelity(
        simulated_perf.energy_map,
        allocation_plan.energy_target
    )

    if energy_efficiency >= 0.95:
        return allocation_plan
    else:
        # Repeat with adjusted parameters
        return optimize_resources_with_simulation(cortex, workload_history)
```

### Pattern 3: Threat Prediction with Multi-Agent Consensus

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RND1    â”‚ Predict next attack vector
â”‚ (Predictor) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ attack_prediction
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UserLM    â”‚ Simulate adversary behavior
â”‚ (Adversary) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ adversary_trace
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ACE     â”‚ Estimate attack impact
â”‚ (Ensemble)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ impact_assessment
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cortex    â”‚ Multi-agent consensus decision
â”‚ (Decision)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ threat_level + recommended_action
       â–¼
     [Alert / Defend]
```

**Code Example**:
```python
# Threat Prediction with Multi-Agent Consensus
def predict_and_respond_to_threat(cortex: TrifectaCortex, current_state: Dict):
    # Step 1: RND1 predicts next attack
    attack_pred = cortex.rnd1.predict_attack(current_state, horizon=10)

    # Step 2: UserLM simulates adversary
    adversary = cortex.userlm.generate_adversarial_persona(
        attack_vector=attack_pred.attack_type
    )
    adversary_trace = cortex.userlm.simulate_behavior(adversary, current_state)

    # Step 3: ACE estimates impact using Shadow Ensemble
    impact_scenarios = cortex.ace.ensemble_predict(adversary_trace.energy_signature)
    impact_consensus, agreement, valid = cortex.ace.shadow_ensemble.ensemble_consensus(
        impact_scenarios
    )

    # Step 4: Multi-agent consensus decision
    threat_level = cortex.coordinate_decision(
        decision_type="threat_assessment",
        candidates=[
            {"threat": "low", "action": "monitor"},
            {"threat": "medium", "action": "alert"},
            {"threat": "high", "action": "defend"}
        ]
    )

    return threat_level
```

---

## ğŸ“Š INTEGRATION WITH EXISTING SYSTEMS

### Integration with Bridge API

**Bridge API Endpoint**: `/trifecta/predict`

```python
# File: src/bridge_api/routes/trifecta_routes.py

from src.trifecta.cortex.trifecta_cortex import TrifectaCortex

@app.post("/trifecta/predict")
async def trifecta_predict(request: TrifectaRequest):
    """
    Multi-agent Trifecta prediction

    Request:
    {
        "mode": "defense" | "optimization" | "threat_prediction",
        "context": {...},
        "agents": ["userlm", "rnd1", "ace"]  # Which agents to use
    }

    Response:
    {
        "prediction": {...},
        "confidence": 0.92,
        "agent_contributions": {
            "userlm": {...},
            "rnd1": {...},
            "ace": {...}
        }
    }
    """
    # Initialize Trifecta Cortex
    cortex = get_trifecta_cortex()

    if request.mode == "defense":
        result = cortex.execute_playbook("red_team_defense_test", request.context)
    elif request.mode == "optimization":
        result = cortex.execute_playbook("resource_optimization", request.context)
    elif request.mode == "threat_prediction":
        result = predict_and_respond_to_threat(cortex, request.context)

    return TrifectaResponse(**result)
```

### Integration with EIL (Phase 5)

**EIL Decision Fusion** â†’ **Trifecta Orchestration**

```python
# File: src/core_ai_layer/eil/core/energy_intelligence_layer.py (EXTEND)

from src.trifecta.cortex.trifecta_cortex import TrifectaCortex

class EnergyIntelligenceLayer:
    # ... existing implementation ...

    def __init__(self, config: EILConfig):
        # ... existing code ...

        # NEW: Add Trifecta Cortex for multi-agent orchestration
        self.trifecta = TrifectaCortex(
            ace_agent=self.ace_agent,  # Existing ACE from Phase 4
            userlm=PersonaGenerator(),  # NEW
            rnd1=ResourceOptimizer()    # NEW
        )

    def decide_with_trifecta(
        self,
        context: EILContext,
        mode: str = "defense"
    ) -> Dict[str, Any]:
        """
        Enhanced decision-making using full Trifecta

        Original EIL: Statistical (40%) + Physics (60%) fusion
        Enhanced EIL: Add Trifecta multi-agent reasoning
        """
        # Original EIL decision
        eil_decision = self.decide(context)

        # Trifecta enhancement
        if mode == "defense":
            # Use UserLM to simulate adversarial scenarios
            # Use RND1 to optimize defense allocation
            # Use ACE to validate confidence
            trifecta_result = self.trifecta.execute_playbook(
                "threat_assessment",
                {"eil_decision": eil_decision, "context": context}
            )
            eil_decision.update(trifecta_result)

        return eil_decision
```

### Integration with AI Shield v2

**AI Shield Layer 4 (Behavioral Safety)** â†’ **Trifecta Red-Team Testing**

```python
# File: src/ai_shield_v2/layers/behavioral_safety.py

from src.trifecta.userlm.red_team_agent import RedTeamAgent

class BehavioralSafetyLayer:
    def __init__(self):
        # ... existing code ...

        # NEW: Red-team agent for adversarial testing
        self.red_team = RedTeamAgent(userlm, ace_agent)

    def validate_behavior(self, behavior: Dict) -> bool:
        """Validate behavior is non-adversarial"""
        # Generate red-team attack scenarios
        attack_scenarios = self.red_team.generate_attack_scenarios(
            behavior_type=behavior["type"]
        )

        # Test behavior against known attacks
        for scenario in attack_scenarios:
            if self._matches_attack_pattern(behavior, scenario):
                return False  # Reject adversarial behavior

        return True  # Behavior passes red-team tests
```

---

## ğŸ“ FILE STRUCTURE

```
src/
â”œâ”€â”€ trifecta/                          # NEW: Trifecta multi-agent system
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ userlm/                        # UserLM (User Language Model)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ persona_generator.py      # Generate synthetic personas
â”‚   â”‚   â”œâ”€â”€ behavior_simulator.py     # Simulate user behavior
â”‚   â”‚   â”œâ”€â”€ red_team_agent.py         # Adversarial red-team testing
â”‚   â”‚   â”œâ”€â”€ dialogue_engine.py        # Multi-turn conversation
â”‚   â”‚   â”œâ”€â”€ intent_classifier.py      # Intent classification
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ persona_model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ behavior_patterns.py
â”‚   â”‚   â”‚   â””â”€â”€ attack_vectors.py
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”‚       â”œâ”€â”€ test_persona_generator.py
â”‚   â”‚       â”œâ”€â”€ test_behavior_simulator.py
â”‚   â”‚       â””â”€â”€ test_red_team_agent.py
â”‚   â”‚
â”‚   â”œâ”€â”€ rnd1/                          # RND1 (Resource + Defense Optimizer)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ resource_optimizer.py     # Optimize resource allocation
â”‚   â”‚   â”œâ”€â”€ defense_evolver.py        # Evolve defense strategies
â”‚   â”‚   â”œâ”€â”€ attack_predictor.py       # Predict attack vectors
â”‚   â”‚   â”œâ”€â”€ evolutionary_engine.py    # Evolutionary algorithm core
â”‚   â”‚   â”œâ”€â”€ fitness_functions.py      # Multi-objective fitness
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ resource_model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ defense_library.py
â”‚   â”‚   â”‚   â””â”€â”€ attack_patterns.py
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”‚       â”œâ”€â”€ test_resource_optimizer.py
â”‚   â”‚       â”œâ”€â”€ test_defense_evolver.py
â”‚   â”‚       â””â”€â”€ test_attack_predictor.py
â”‚   â”‚
â”‚   â””â”€â”€ cortex/                        # Trifecta Cortex (Coordination Hub)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ trifecta_cortex.py        # Main coordination hub
â”‚       â”œâ”€â”€ memory_manager.py         # Shared memory management
â”‚       â”œâ”€â”€ playbook_engine.py        # Playbook execution
â”‚       â”œâ”€â”€ decision_arbiter.py       # Multi-agent decision arbitration
â”‚       â””â”€â”€ tests/
â”‚           â”œâ”€â”€ test_trifecta_cortex.py
â”‚           â””â”€â”€ test_playbook_engine.py
â”‚
â”œâ”€â”€ core_ai_layer/                     # EXISTING: Thermodynasty
â”‚   â”œâ”€â”€ nvp/                           # Phase 4: ACE (already exists)
â”‚   â”‚   â””â”€â”€ ace/
â”‚   â”‚       â”œâ”€â”€ ace_agent.py           # âœ… REUSE in Trifecta
â”‚   â”‚       â””â”€â”€ shadow_ensemble.py     # âœ… REUSE in Trifecta
â”‚   â””â”€â”€ eil/                           # Phase 5: EIL (already exists)
â”‚       â””â”€â”€ core/
â”‚           â””â”€â”€ energy_intelligence_layer.py  # EXTEND with Trifecta
â”‚
â””â”€â”€ bridge_api/                        # Bridge API (TO BE CREATED)
    â””â”€â”€ routes/
        â””â”€â”€ trifecta_routes.py         # NEW: Trifecta API endpoints
```

---

## ğŸš€ IMPLEMENTATION ROADMAP

### Phase 1: Foundation (Weeks 1-2)

**Week 1: UserLM Core**
- [ ] Implement `PersonaGenerator` class
  - Integrate OpenAI/Anthropic LLM for persona generation
  - Create persona library with 100+ templates
  - Add persistence (Neo4j graph database)
- [ ] Implement `BehaviorSimulator` class
  - Integrate with existing ACE agent for behavior prediction
  - Add thermodynamic signature generation
  - Implement anomaly detection using ACE calibration
- [ ] Write tests (pytest)
  - `test_persona_generator.py`: 20+ test cases
  - `test_behavior_simulator.py`: 15+ test cases

**Week 2: RND1 Core**
- [ ] Implement `ResourceOptimizer` class
  - Evolutionary algorithm (NSGA-II for multi-objective)
  - Integration with ACE for workload prediction
  - Energy-aware fitness function
- [ ] Implement `DefenseEvolver` class
  - Defense strategy representation
  - Fitness evaluation with red-team testing
  - Evolutionary operators (crossover, mutation, selection)
- [ ] Write tests (pytest)
  - `test_resource_optimizer.py`: 15+ test cases
  - `test_defense_evolver.py`: 20+ test cases

### Phase 2: Integration (Weeks 3-4)

**Week 3: Trifecta Cortex**
- [ ] Implement `TrifectaCortex` class
  - Shared memory management
  - Inter-agent communication protocol
  - Playbook engine
- [ ] Implement `MemoryManager`
  - Persistent storage (Neo4j)
  - Memory retrieval and indexing
- [ ] Implement `PlaybookEngine`
  - Define 5 core playbooks:
    1. Red-team defense test
    2. Resource optimization
    3. Threat prediction
    4. Adversarial simulation
    5. Defense evolution
- [ ] Write tests (pytest)
  - `test_trifecta_cortex.py`: 25+ test cases

**Week 4: Advanced Components**
- [ ] Implement `RedTeamAgent` (UserLM)
  - Attack scenario generation
  - MITRE ATT&CK integration
  - Attack simulation
- [ ] Implement `AttackPredictor` (RND1)
  - Attack trajectory prediction using ACE NVP
  - Threat landscape analysis
- [ ] Implement `DecisionArbiter` (Cortex)
  - Multi-agent voting
  - Weighted consensus
  - Confidence aggregation
- [ ] Write tests (pytest)
  - `test_red_team_agent.py`: 20+ test cases
  - `test_attack_predictor.py`: 15+ test cases
  - `test_decision_arbiter.py`: 20+ test cases

### Phase 3: System Integration (Weeks 5-6)

**Week 5: EIL Integration**
- [ ] Extend `EnergyIntelligenceLayer` with Trifecta
- [ ] Add `decide_with_trifecta()` method
- [ ] Integration tests with existing EIL tests
- [ ] Performance benchmarking

**Week 6: Bridge API Integration**
- [ ] Implement `/trifecta/predict` endpoint
- [ ] Implement `/trifecta/red-team` endpoint
- [ ] Implement `/trifecta/optimize` endpoint
- [ ] API documentation (OpenAPI spec)
- [ ] End-to-end integration tests

### Phase 4: Testing & Optimization (Week 7-8)

**Week 7: Comprehensive Testing**
- [ ] Unit tests: 200+ tests across all components
- [ ] Integration tests: 50+ tests
- [ ] End-to-end tests: 20+ scenarios
- [ ] Performance benchmarks
  - Latency targets: <100ms per Trifecta decision
  - Throughput targets: >1000 req/s
  - Memory targets: <4GB per Cortex instance

**Week 8: Optimization & Documentation**
- [ ] Performance optimization (profiling, caching)
- [ ] Documentation
  - API documentation
  - Architecture diagrams
  - Integration guides
  - Playbook examples
- [ ] Deployment configs (Kubernetes)

---

## ğŸ“Š SUCCESS METRICS

### Functional Metrics
- âœ… UserLM generates 100+ diverse personas
- âœ… BehaviorSimulator achieves >90% realism (human evaluation)
- âœ… RedTeamAgent generates 50+ attack scenarios covering MITRE ATT&CK
- âœ… ResourceOptimizer achieves >20% efficiency improvement
- âœ… DefenseEvolver evolves defenses with >95% attack detection rate
- âœ… AttackPredictor achieves >80% prediction accuracy (AUC-ROC)
- âœ… TrifectaCortex coordinates 3 agents with <100ms latency

### Integration Metrics
- âœ… Seamless integration with existing ACE agent (no breaking changes)
- âœ… EIL enhanced with Trifecta shows >10% decision accuracy improvement
- âœ… Bridge API exposes Trifecta endpoints with 100% uptime
- âœ… 200+ unit tests, 50+ integration tests, all passing

### Performance Metrics
- âœ… Trifecta decision latency: <100ms (p95)
- âœ… Resource optimization runtime: <1s per optimization
- âœ… Defense evolution: <5s per generation
- âœ… Memory footprint: <4GB per Cortex instance
- âœ… Throughput: >1000 Trifecta requests/second

---

## ğŸ”’ SECURITY CONSIDERATIONS

### 1. UserLM Red-Team Isolation
- Red-team attack simulations run in isolated sandboxes
- No real attacks executed, only simulations
- Attack vectors stored in encrypted database

### 2. RND1 Defense Evolution
- Evolved defenses validated before deployment
- Multi-layer approval (ACE confidence + human review)
- Rollback mechanism for ineffective defenses

### 3. Trifecta Cortex Access Control
- API endpoints protected by UTID + Proof validation
- Agent coordination logs auditable
- Memory access restricted by RBAC

---

## ğŸ“– REFERENCES

### Existing Code References
- ACE Agent: `src/core_ai_layer/nvp/ace/ace_agent.py:407-530`
- Shadow Ensemble: `src/core_ai_layer/nvp/ace/shadow_ensemble.py:74-100`
- EIL Core: `src/core_ai_layer/eil/core/energy_intelligence_layer.py`

### Related Documents
- `COMPREHENSIVE_INTEGRATION_ANALYSIS.md` - Master integration plan
- `FINAL_FORM_ARCHITECTURE.md` - Complete system architecture
- `DEVELOPMENT_LINEAGE.md` - Evolution from prototypes to production

### External References
- MITRE ATT&CK Framework: https://attack.mitre.org/
- NSGA-II Evolutionary Algorithm: Deb et al., 2002
- Byzantine Fault Tolerance: Castro & Liskov, 1999

---

**Status**: Ready for Implementation âœ…
**Priority**: High - Foundation for Expansion Packs 1, 4, 5
**Dependencies**: Existing ACE agent, Bridge API (in parallel)
**Estimated Effort**: 8 weeks (2 engineers)

**Date**: November 21, 2025
**Created By**: Industriverse Core Team (Claude Code)
