# Production Training Configuration
# The Big Burn - GenN-1 (Sovereign-Alpha)

experiment_name: "sovereign_alpha_1_big_burn"
model_name: "Sovereign-Alpha-1"

# Data
data:
  vault_path: "/data/fossil_vault" # Path inside Docker container
  sequence_length: 8192
  batch_size: 4096 # Global batch size (across all GPUs)
  num_workers: 16

# Model Architecture (EBDM)
model:
  hidden_dim: 4096
  num_layers: 32
  num_heads: 32
  vocab_size: 32000

# Training
training:
  epochs: 1
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_steps: 2000
  gradient_accumulation_steps: 4
  mixed_precision: "bf16" # bfloat16 for H100

# Checkpointing
checkpoint:
  save_every_steps: 1000
  keep_last_n: 5
  upload_to_b2: true

# Physics Constraints
physics:
  enforce_thermodynamics: true
  entropy_penalty_weight: 0.01
