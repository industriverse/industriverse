# Production Training Configuration
# The Big Burn - GenN-1 (Sovereign-Alpha)

experiment_name: "sovereign_alpha_1_big_burn"
model_name: "Sovereign-Alpha-1"

# Data
data:
  vault_path: "/data/fossil_vault" # Path inside Docker container
  sequence_length: 4096 # Reduced to save memory
  batch_size: 2 # Reduced to 2 to fit in memory
  num_workers: 4

# Model Architecture (EBDM)
model:
  hidden_dim: 4096
  num_layers: 32
  num_heads: 32
  vocab_size: 32000
  use_checkpointing: true # Enable gradient checkpointing

# Training
training:
  epochs: 1
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_steps: 2000
  gradient_accumulation_steps: 64 # Effective batch size = 512
  mixed_precision: "bf16" # bfloat16 for H100

# Checkpointing
checkpoint:
  save_every_steps: 1000
  keep_last_n: 5
  upload_to_b2: true

# Physics Constraints
physics:
  enforce_thermodynamics: true
  entropy_penalty_weight: 0.01
