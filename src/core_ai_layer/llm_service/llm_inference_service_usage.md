# LLMInferenceService Usage Guide

## Introduction

The `LLMInferenceService` is a crucial component within the Core AI Layer of the Industriverse platform. Its primary responsibility is to orchestrate and execute inference requests for various Large Language Models (LLMs) that are managed by the `LLMModelManager`. This service is designed to be robust, flexible, and performant, catering to diverse use cases ranging from real-time interactive chat applications to batch processing tasks like report generation and data summarization.

This document provides a comprehensive guide on how to utilize the `LLMInferenceService`, detailing its functionalities, request/response schemas, configuration options, error handling mechanisms, and integration points. It aims to equip developers with the necessary knowledge to effectively leverage this service for building AI-powered features within Industriverse applications.

## Core Functionalities

The `LLMInferenceService` offers several core functionalities:

1.  **Model Agnostic Inference:** While initially focusing on Hugging Face Transformer models, the service is designed with an extensible handler architecture. This allows for the future integration of different model types and runtimes (e.g., GGUF for llama.cpp, specialized serving frameworks like vLLM or Triton Inference Server) by implementing new `InferenceHandler` interfaces. The service automatically selects the appropriate handler based on the loaded model's characteristics.
2.  **Synchronous and Asynchronous Inference:** It supports both blocking (synchronous) inference for tasks where an immediate response is required and non-blocking (asynchronous) streaming inference. Streaming is particularly useful for interactive applications, providing token-by-token output to enhance user experience.
3.  **Dynamic Parameter Configuration:** Inference requests can specify a rich set of generation parameters, such as `max_new_tokens`, `temperature`, `top_p`, `top_k`, `repetition_penalty`, and `do_sample`. These parameters allow fine-grained control over the LLM's output generation process, tailoring it to specific needs.
4.  **Stop Sequence Handling:** Users can define custom stop sequences within an inference request. The service, through its handlers, will monitor the generated output and terminate generation when one of these sequences is encountered, providing cleaner and more controlled outputs.
5.  **Integration with LLMModelManager:** The service seamlessly integrates with the `LLMModelManager` to retrieve loaded model instances. This ensures that model lifecycle management (loading, unloading, resource allocation) is handled centrally and efficiently.
6.  **Comprehensive Error Handling:** The service implements robust error handling, distinguishing between model loading issues (e.g., model not found, resource not available), inference execution errors, and configuration problems. Errors are clearly reported in the `InferenceResponse` or through the error stream.
7.  **Request Tracking:** Each inference request is assigned a unique ID (either provided by the client or generated by the service) for logging and tracking purposes, facilitating debugging and monitoring.

## Request and Response Schemas

The service utilizes Pydantic models for defining the structure of inference requests and responses, ensuring type safety and clear data contracts.

### `GenerationParams`

This model defines the parameters that control the LLM's generation process:

*   `max_new_tokens` (Optional[int], default: 256): The maximum number of new tokens to be generated by the model.
*   `temperature` (Optional[float], default: 0.7, range: 0.0-2.0): Controls the randomness of the output. Lower values make the output more deterministic, while higher values increase randomness.
*   `top_p` (Optional[float], default: None, range: 0.0-1.0): Nucleus sampling parameter. If set, only tokens with a cumulative probability mass up to `top_p` are considered for generation.
*   `top_k` (Optional[int], default: None, range: >=0): Top-k sampling parameter. If set, only the `top_k` most probable tokens are considered at each step.
*   `repetition_penalty` (Optional[float], default: None, range: >=1.0): Penalizes tokens that have already appeared in the context, discouraging repetition.
*   `do_sample` (Optional[bool], default: True): If True, sampling strategies (like temperature, top_p, top_k) are used. If False, greedy decoding is performed.
*   `num_beams` (Optional[int], default: 1): Number of beams to use for beam search. A value of 1 means no beam search.
*   `early_stopping` (Optional[bool], default: False): Relevant for beam search. If True, generation stops when all beam hypotheses have reached the EOS token.

### `InferenceRequest`

This model defines the structure of an inference request sent to the service:

*   `model_id` (str): The unique identifier of the LLM to be used for inference, as registered with the `LLMModelManager`.
*   `prompt` (str): The input text prompt for the LLM.
*   `params` (Optional[GenerationParams], default: default `GenerationParams`): An instance of `GenerationParams` to control the generation process. If not provided, default values are used.
*   `stream` (bool, default: False): If True, the service will attempt to stream the response token by token. If False, a single consolidated response is returned after generation is complete.
*   `request_id` (str, default: auto-generated UUID): A unique identifier for the request. Clients can provide their own ID for tracking.
*   `stop_sequences` (Optional[List[str]], default: None): A list of strings. If any of these sequences appear in the generated output, generation will stop.

### `InferenceResponse`

This model defines the structure of the response returned by the service for non-streaming requests, or as a final summary for streaming requests (though streaming primarily yields token strings):

*   `request_id` (str): The unique ID of the corresponding request.
*   `model_id` (str): The ID of the model used for inference.
*   `generated_text` (Optional[str], default: None): The text generated by the LLM. This is None if an error occurred before generation could complete.
*   `error_message` (Optional[str], default: None): If an error occurred, this field contains a descriptive error message.
*   `finish_reason` (Optional[str], default: None): Indicates why the generation finished (e.g., "length" if `max_new_tokens` was reached, "stop_sequence" if a stop sequence was encountered, "error" if an error occurred).
*   `prompt_tokens` (Optional[int], default: None): The number of tokens in the input prompt.
*   `generated_tokens` (Optional[int], default: None): The number of tokens generated by the model.
*   `inference_time_ms` (Optional[float], default: None): The time taken for the inference process, in milliseconds.

## Using the `LLMInferenceService`

### Initialization

To use the `LLMInferenceService`, it first needs to be instantiated. It requires an instance of `LLMModelManager` to be passed during initialization, as it relies on the manager to access loaded model instances.

```python
# Assuming llm_model_manager is an initialized instance of LLMModelManager
# and LLMInferenceService is imported

from .llm_model_manager import LLMModelManager # Or appropriate import path
from .llm_inference_service import LLMInferenceService, InferenceRequest, GenerationParams

# Example: Initialize LLMModelManager (details depend on its implementation)
# model_manager = LLMModelManager(config_path="path/to/models.yaml")
# await model_manager.load_configured_models() # Ensure models are loaded

# Initialize the inference service
# inference_service = LLMInferenceService(model_manager=llm_model_manager)
```

Note: The example above assumes a placeholder `LLMModelManager`. In a real scenario, `llm_model_manager` would be a fully functional instance.

### Making a Synchronous Inference Request

For tasks where a complete response is needed at once, a synchronous (non-streaming) request can be made. The `process_inference` method is used for this, with `stream` set to `False` (which is the default).

```python
async def make_synchronous_request(inference_service: LLMInferenceService):
    request_params = GenerationParams(
        max_new_tokens=100,
        temperature=0.8
    )
    sync_request = InferenceRequest(
        model_id="your-model-id", # Replace with an actual loaded model ID
        prompt="Translate the following English text to French: Hello, how are you?",
        params=request_params,
        stream=False,
        request_id="my-sync-req-001"
    )

    try:
        response = await inference_service.process_inference(sync_request)
        if response.error_message:
            print(f"Error: {response.error_message}")
        else:
            print(f"Generated Text: {response.generated_text}")
            print(f"Finish Reason: {response.finish_reason}")
            print(f"Inference Time: {response.inference_time_ms} ms")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

# await make_synchronous_request(inference_service)
```

### Making a Streaming Inference Request

For interactive applications, streaming inference provides a much better user experience by delivering tokens as they are generated. To make a streaming request, set `stream` to `True` in the `InferenceRequest`. The `process_inference` method will then return an asynchronous generator.

```python
async def make_streaming_request(inference_service: LLMInferenceService):
    stream_request = InferenceRequest(
        model_id="your-model-id", # Replace with an actual loaded model ID
        prompt="Write a short story about a robot who discovers music.",
        stream=True,
        params=GenerationParams(max_new_tokens=500, temperature=0.9),
        request_id="my-stream-req-001"
    )

    full_response_text = ""
    try:
        async for token_chunk in await inference_service.process_inference(stream_request):
            if isinstance(token_chunk, str) and token_chunk.startswith("STREAM_ERROR:"):
                print(f"\nError during stream: {token_chunk}")
                break
            elif isinstance(token_chunk, str):
                print(token_chunk, end="", flush=True)
                full_response_text += token_chunk
        print("\n--- Stream Complete ---")
        # Note: A final InferenceResponse object is not typically yielded by the stream itself.
        # The stream yields token strings or error markers.
    except Exception as e:
        print(f"\nAn unexpected error occurred during streaming: {e}")

# await make_streaming_request(inference_service)
```

### Handling Stop Sequences

Stop sequences can be provided in the `InferenceRequest` to terminate generation when specific text patterns are encountered. The `HuggingFaceTransformersHandler` (and any other compliant handler) will respect these sequences.

```python
async def make_request_with_stop_sequence(inference_service: LLMInferenceService):
    request_with_stop = InferenceRequest(
        model_id="your-model-id",
        prompt="List three primary colors. 1.",
        params=GenerationParams(max_new_tokens=50),
        stop_sequences=["\n4.", "End of list."], # Stop if it tries to list a 4th or says "End of list."
        request_id="stop-seq-req-001"
    )
    response = await inference_service.process_inference(request_with_stop)
    if response.error_message:
        print(f"Error: {response.error_message}")
    else:
        print(f"Generated Text: {response.generated_text}")
        print(f"Finish Reason: {response.finish_reason}") # Should be 'stop_sequence'

# await make_request_with_stop_sequence(inference_service)
```

## Error Handling

The `LLMInferenceService` is designed to handle various error conditions gracefully:

1.  **Model Not Found:** If the `model_id` specified in the request does not correspond to a model known by the `LLMModelManager`, a `ModelNotFoundError` will be caught. For synchronous requests, the `InferenceResponse` will contain an appropriate `error_message`. For streaming requests, a "STREAM_ERROR: Model not found..." message will be yielded, or the exception might be raised before the stream begins depending on the exact point of failure.
2.  **Resource Not Available:** If the `LLMModelManager` indicates that resources (e.g., GPU memory) are insufficient to load or use the requested model, a `ResourceNotAvailableError` is handled. Similar to model not found errors, this is reported in the `error_message` or as a "STREAM_ERROR".
3.  **Configuration Error:** If there's an issue with the model's configuration that prevents inference, a `ConfigurationError` might be raised and handled.
4.  **Inference Execution Errors:** Errors occurring within the inference handler (e.g., issues during model.generate()) are caught. These are reported as `InferenceError` or general exceptions. The `error_message` field or a "STREAM_ERROR" will detail the issue.
5.  **No Suitable Handler:** If a loaded model is of a type for which no registered `InferenceHandler` can process it, an `InferenceError` is raised indicating this mismatch.

It is crucial for client applications to check the `error_message` field in synchronous responses and to monitor for "STREAM_ERROR:" prefixes in streaming responses to handle failures appropriately.

## Extending with New Handlers

The service can be extended to support new types of models or inference runtimes by creating a new class that inherits from `InferenceHandler` and implementing its abstract methods:

*   `can_handle(self, loaded_model: LoadedModel) -> bool`: This method should return `True` if the handler is capable of performing inference with the given `LoadedModel` instance (e.g., by checking the type of `loaded_model.model_object` or `loaded_model.config`).
*   `async def infer(self, loaded_model: LoadedModel, request: InferenceRequest) -> InferenceResponse`: Implements the logic for synchronous inference.
*   `async def stream_infer(self, loaded_model: LoadedModel, request: InferenceRequest) -> AsyncGenerator[str, None]`: Implements the logic for streaming inference.

Once a new handler is implemented, it needs to be registered with the `LLMInferenceService` instance, typically during initialization or via a dedicated registration method (if added to the service).

```python
# Example of a new handler (conceptual)
# class MyCustomHandler(InferenceHandler):
#     def can_handle(self, loaded_model: LoadedModel) -> bool:
#         # Logic to check if this handler can manage the model
#         return isinstance(loaded_model.model_object, MyCustomModelType)

#     async def infer(self, loaded_model: LoadedModel, request: InferenceRequest) -> InferenceResponse:
#         # Implementation for synchronous inference
#         pass

#     async def stream_infer(self, loaded_model: LoadedModel, request: InferenceRequest) -> AsyncGenerator[str, None]:
#         # Implementation for streaming inference
#         # yield token_chunk
#         pass

# Registering the new handler (assuming inference_service has a method like register_handler)
# my_custom_handler = MyCustomHandler()
# inference_service.register_handler(my_custom_handler)
```

The `LLMInferenceService` iterates through its registered handlers in the order they were added, using the first one for which `can_handle` returns `True`.

## Logging

The service uses the standard Python `logging` module. Logs provide insights into request processing, handler selection, generation parameters, inference times, and errors. Ensure that the logging infrastructure of the Industriverse platform is configured to capture and manage these logs effectively.

## Conclusion

The `LLMInferenceService` provides a robust and flexible way to perform inference with various Large Language Models within the Industriverse Core AI Layer. By understanding its request/response schemas, error handling, and extensibility, developers can effectively integrate advanced AI capabilities into their applications. Always refer to the latest version of this documentation and the source code for the most up-to-date information.

